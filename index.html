<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=950">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        para {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 18px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Ziyi Yang</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/png" href="./images/hku.png">
</head>

<body>
<table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <name>Ziyi Yang</name>
                        </p>

                        I am a master student at Zhejiang University advised by professor <a
                            href="http://www.cad.zju.edu.cn/home/jin/">Xiaogang
                        Jin</a>.
<!--                         My research lies at the neural rendering, inverse rendering, and computer graphics.
                        <!--  (Please don't say I'm a computer vision researcher. I am truly a graphics enthusiast lol ðŸ˜‚). -->
                        I am currently interested in 3D Gaussian Splatting (But I am not optimistic about it). -->
                        My research lies at the inverse rendering, and computer graphics.
                        (Please don't say I'm a computer vision researcher. I am truly a graphics enthusiast lol ðŸ˜‚). 
                        I am currently interested in 3D Gaussian Splatting (But I am not optimistic about it).
<!--                         Now, I am having a very pleasant time at ByteDance MMLab as a research intern. -->
                        Prior to joining ZJU, I got my Bachelor's Degree from <a href="https://en.sjtu.edu.cn/">Shanghai
                        Jiao Tong University</a> in 2022.

                        </br></br>
                        <strong><font color="red">I'm now looking for a 25 Fall Ph.D. position in computer graphics / 3D
                            vision! Welcome to contact me! </font></strong>

                        </br>
                        </p>
                        <p align="center">
                            <a href="mailto:14ziyiyang@gmail.com"> Email </a> /
                            <a href="https://scholar.google.com/citations?user=B0IyfqQAAAAJ&hl=en">
                                Google Scholar </a> /
                            <a href="https://github.com/ingra14m"> Github </a>
                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./images/yzy.jpg"
                                           style="width: 150; height: 225;"></td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Recent News</heading>
                    </td>
                </tr>
                </tbody>
            </table>
            <div style="height:170px;overflow-y:scroll;">
                <ul>
                    <li>[2025.02] two papers have been accepted by the <b>CVPR 2025</b>.
                    </li>
                </ul>
                <ul>
                    <li>[2024.09] two papers have been accepted by the <b>NeurIPS 2024</b>.
                    </li>
                </ul>
                <ul>
                    <li>[2024.07] one paper has been accepted by the <b>SIGGRAPH Asia 2024 (TOG)</b>.
                    </li>
                </ul>
                <ul>
                    <li>[2024.02] two papers have been accepted by the <b>CVPR 2024</b>.
                    </li>
                </ul>
                <ul>
                    <li>[2023.12] one paper has been accepted by the <b>AAAI 2024</b>.
                    </li>
                </ul>
            </div>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <td>
                    <heading>Publications</heading>
                </td>
                <tr>
                    <td width="20%"><img src="./publications/2024/Spec-Gaussian/teaser.png" alt="PontTuset"
                                         width="300" height="160" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ingra14m.github.io/Spec-Gaussian-website/">
                            <papertitle>Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=Specular-Gaussians&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a><br><strong>Ziyi Yang</strong>, Xinyu Gao, Yang-Tian Sun, Yi-Hua Huang, Xiaoyang Lyu, Wen
                            Zhou, Shaohui Jiao, Xiaojuan Qi<sup>&dagger;</sup>, Xiaogang Jin<sup>&dagger;</sup>
                            <!--                                <a href="https://xjqi.github.io/">Xiaojuan Qi</a>-->
                            <br>
                            <em><strong>NeurIPS</strong></em>, 2024.
                            <br>
                            <a href="https://arxiv.org/abs/2402.15870">Paper</a> /
                            <a href="https://ingra14m.github.io/Spec-Gaussian-website/">Project Page</a> /
                            <a href="https://github.com/ingra14m/Specular-Gaussians">Code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">Spec-Gaussian aims to tackle scenes with specular
                            highlights and anisotropy. The key idea is to employ the ASG appearance field instead of SH
                            to model the appearance of 3D Gaussian.</p>
                    </td>
                </tr>

                <tr>
                    <td width="20%"><img src="./publications/2024/sire-ir/teaser.png" alt="PontTuset"
                                         width="300" height="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ingra14m.github.io/RobIR_website">
                            <papertitle>RobIR: Robust Inverse Rendering for High-Illumination Scenes
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=RobIR&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br><strong>Ziyi Yang</strong>, Yanzhen Chen, Xinyu Gao, Yazhen Yuan, Yu Wu, Xiaowei Zhou,
                            Xiaogang Jin<sup>&dagger;</sup>
                            <br>
                            <em><strong>NeurIPS</strong></em>, 2024.
                            <br>
                            <a href="https://ingra14m.github.io/RobIR_website">Project Page</a> /
                            <a href="https://arxiv.org/abs/2310.13030">arXiv</a> /
                            <a href="https://github.com/ingra14m/RobIR">code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a novel neural field-based inverse
                            rendering framework for high-illumination scenes. We employ a scene-specific ACES tone
                            mapping and regularized visibility estimation to eliminate the shadow in the PBR
                            materials.</p>
                    </td>
                </tr>

                <tr>
                    <td width="20%"><img src="./publications/2024/3DGSR/teaser.jpg" alt="PontTuset"
                                         width="300" height="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/abs/2404.00409">
                            <papertitle>3DGSR: Implicit Surface Reconstruction with 3D Gaussian Splatting
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=?&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br>Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, <strong>Ziyi Yang</strong>, Yilun
                            Chen, Jiangmiao Pang, Xiaojuan Qi<sup>&dagger;</sup>
                            <br>
                            <em><strong>SIGGRAPH Asia (TOG)</strong></em>, 2024.
                            <br>
                            <a href="">Project Page</a> /
                            <a href="https://arxiv.org/abs/2404.00409">arXiv</a> /
                            <a href="">code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a joint reconstruction technique coupling a
                            GS and neural SDFs to achieve high quality reconstructions.</p>
                    </td>
                </tr>

                <tr>
                    <td width="20%"><img src="./publications/2024/SC-GS/teaser.png" alt="PontTuset" width="300"
                                         height="111" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://yihua7.github.io/SC-GS-web/">
                            <papertitle>SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic
                                Scenes
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=yihua7&repo=SC-GS&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>

                            <br>Yi-Hua Huang<sup>*</sup>, Yang-Tian Sun<sup>*</sup>, <strong>Ziyi
                                Yang<sup>*</sup></strong>, Xiaoyang Lyu,
                            Yan-Pei Cao<sup>&dagger;</sup>, Xiaojuan Qi<sup>&dagger;</sup>
                            <br>
                            <em><strong>CVPR</strong></em>, 2024.
                            <br>
                            <a href="https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf">Paper</a>
                            /
                            <a href="https://yihua7.github.io/SC-GS-web/">Project Page</a> /
                            <a href="https://github.com/yihua7/SC-GS">Code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px"> We propose a new representation that
                            explicitly decomposes the motion
                            and appearance of dynamic scenes into sparse control points and dense Gaussians,
                            respectively.
                            Our key idea is to use sparse control points, significantly fewer in number than
                            the Gaussians,
                            to learn compact 6 DoF transformation bases, which can be locally interpolated
                            through learned interpolation weights
                            to yield the motion field of 3D Gaussians. Please visit project page for more
                            demos.</p>
                    </td>
                </tr>

                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="25%"><img src="./publications/2024/Deformable-GS/teaser.png" alt="PontTuset" width="300"
                                         height="169" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://github.com/ingra14m/Deformable-Gaussians">
                            <papertitle>Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=Deformable-3D-Gaussians&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br><strong>Ziyi Yang</strong>, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang
                            Jin<sup>&dagger;</sup>
                            <br>
                            <em><strong>CVPR</strong></em>, 2024.
                            <br>
                            <strong><font color="red">Final score: 5, 5, 5 </font></strong>
                            <br>
                            <a href="https://arxiv.org/abs/2309.13101">arXiv</a> /
                            <a href="https://ingra14m.github.io/Deformable-Gaussians/">Project Page</a> /
                            <a href="https://github.com/ingra14m/Deformable-3D-Gaussians">Code</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px"><b>The first deformation-based Gaussian splatting for
                            dynamic scenes.</b> We propose a deformable 3D Gaussian Splatting that reconstructs scenes
                            using 3D Gaussians and learns them in canonical space with a deformation field to model
                            monocular dynamic scenes. We also introduce an annealing smoothing training to mitigate the
                            impact of inaccurate poses in real-world datasets.</p>
                    </td>
                </tr>


                <tr>
                    <td width="25%"><img src="./publications/2024/agif/teaser.jpeg" alt="PontTuset" width="300"
                                         height="168" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="">
                            <papertitle>A General Implicit Framework for Fast NeRF Composition and Rendering
                            </papertitle>
                        </a>
                            <br>Xinyu Gao, <strong>Ziyi Yang</strong>, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin<sup>&dagger;</sup>,
                            Changqing Zou<sup>&dagger;</sup>
                            <br>
                            <em><strong>AAAI</strong></em>, 2024.
                            <br>
                            <a href="https://arxiv.org/abs/2308.04669">ArXiv</a>
                            <!--                            <a href="https://github.com/CVMI-Lab/Speech2Lip">Code</a>-->
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a general implicit pipeline for composing
                            NeRF objects quickly. </p>
                    </td>
                </tr>
                <tbody>
            </table>

            <!-- SECTION 5 -->
            <!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td>-->
            <!--                        <heading>Open-source Contribution</heading>-->
            <!--                    </td>-->
            <!--                </tr>-->
            <!--                </tbody>-->
            <!--            </table>-->

            <!--            &lt;!&ndash;SECTION 6 &ndash;&gt;-->
            <!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td width="25%"><img src="./images/ICRA18/robots.jpg" alt="PontTuset" width="300"-->
            <!--                                         style="border-style: none"></td>-->
            <!--                    <td width="75%" valign="top">-->
            <!--                        <p><a href="https://www.robomaster.com/en-US/resource/pages/announcement/728">-->
            <!--                            <papertitle>ICRA 2018 DJI RoboMaster AI Challenge</papertitle>-->
            <!--                        </a>-->
            <!--                            <br><a href="./images/ICRA18/team.jpg">Team: I Hiter</a>. Xingguang Zhong, Xin-->
            <!--                            Kong,-->
            <!--                            <strong>Xiaoyang Lyu</strong>, Le Qi, Hao Huang, Linrui Tian, Songwei Li-->
            <!--                            <br>-->
            <!--                            <em> IEEE International Conference on Robotics and Automation-->
            <!--                                (<strong>ICRA</strong>)</em>,-->
            <!--                            2018. Brisbane, Australia.-->
            <!--                            <br>-->
            <!--                            <a href="./images/ICRA18/champion.jpg"><strong>Global Champion</strong> </a> /-->
            <!--                            <a href="./images/ICRA18/ranking.jpg">Ranking: <strong>1st</strong>/21 </a>/-->
            <!--                            <a href="./images/ICRA18/certificate.jpg">Certificate</a> /-->
            <!--                            <a href="/images/ICRA18/ICRA18-DJI.mp4">Video</a> /-->
            <!--                            <a href="./images/ICRA18/rules.pdf">Rules</a>-->
            <!--                        </p>-->
            <!--                        <p></p>-->
            <!--                        <p align="justify" style="font-size:13px">Our team built two fully automatic robots,-->
            <!--                            including <a href="./images/ICRA18/IHiter-tech.pdf">-->
            <!--                                machinery, circuit, control and algorithm</a>. I was responsible for visual-->
            <!--                            servo, target-->
            <!--                            detection, target localization and-->
            <!--                            decision-making of robots.</p>-->
            <!--                    </td>-->
            <!--                </tr>-->

            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td width="25%"><img src="./images/RM17/autofightback.gif" alt="PontTuset" width="300"-->
            <!--                                         style="border-style: none"></td>-->
            <!--                    <td width="75%" valign="top">-->
            <!--                        <p><a href="https://www.robomaster.com/en-US/robo/history">-->
            <!--                            <papertitle>2017, 2018, 2019 RoboMaster Robotics Competition</papertitle>-->
            <!--                        </a>-->
            <!--                            <br><a href="./images/RM17/team.jpg">Team: I Hiter</a>. Wei Chen, Xin Kong,-->
            <!--                            <strong>Xiaoyang-->
            <!--                                Lyu</strong>, etc.-->
            <!--                            <br>-->
            <!--                            <em>China University Robot Competition (å…¨å›½å¤§å­¦ç”Ÿæœºå™¨äººå¤§èµ›)</em>, 2017, 2018, 2019.-->
            <!--                            Shenzhen, China.-->
            <!--                            <br>-->
            <!--                            <a href="./images/RM17/first-prize.bmp"><strong>First Prize</strong> </a> /-->
            <!--                            <a href="https://en.wikipedia.org/wiki/RoboMaster#Winners">Ranking:-->
            <!--                                <strong>4th</strong>/200+-->
            <!--                            </a> in 2017, 2018, <a-->
            <!--                                    href="https://www.robomaster.com/en-US/resource/pages/announcement/1035"><strong>6th</strong>/200+-->
            <!--                                in 2019.</a>/-->
            <!--                            <a href="./images/RM17/rm_lxy.png">Certificate</a> /-->
            <!--                            <a href="https://www.bilibili.com/video/av27846163/">Highlights</a>-->
            <!--                        </p>-->
            <!--                        <p></p>-->
            <!--                        <p align="justify" style="font-size:13px">Our team built more than <a-->
            <!--                                href="https://www.bilibili.com/video/av23489177/">10 complex automatic or-->
            <!--                            semi-automatic-->
            <!--                            robots</a> every year.-->
            <!--                            In 2017, I was mainly responsible for building and <a-->
            <!--                                    href="images/RM17/manipulator.jfif">manipulating</a> Engineering Robot. In-->
            <!--                            2018, I was-->
            <!--                            responsible for-->
            <!--                            <a href="./images/RM17/visual-servo.mp4">visual servo</a>, which involves-->
            <!--                            computer vision and-->
            <!--                            machine learning. In 2019, I became the leader of computer vision group and the-->
            <!--                            coach of our team.-->
            <!--                        </p>-->
            <!--                    </td>-->
            <!--                </tr>-->
            <!--            </table>-->

            <!-- SECTION 5 -->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tbody><tr>
  <td><heading>Projects</heading>
  </td>
  </tr></tbody>
</table> -->

            <!--SECTION 6 -->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

<tbody><tr>
    <td width="20%"><img src="./images/ST18/strawberry.gif" alt="PontTuset" width="250" style="border-style: none"></td>
    <td width="80%" valign="top">
        <papertitle>Flowing Strawberry Picking Robot</papertitle>
        <br><strong>Xiaoyang Lyu</strong>, Le Qi, Linrui Tian, Songwei Li
        <br>
        <em> Undergraduate Mechanical Innovation Competition.
        <br>

        <strong>First Prize</strong> /
        <a href="./images/ST18/certificates.jpg">Certificate</a>
        </p><p></p>
        <p align="justify" style="font-size:13px"> Our group build a strawberry picking robot,
            which can detect strawberries and pick them automatically. I responsible for designing
            a machine learning algorithm which can detect strawberry on embedded platform.</td>
</tr>

</table> -->

            <!--SECTION 7 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Open-source Contribution</heading>
                        <p align="justify">
                            <para><b>1. depth-diff-gaussian-rasterization</b></para>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=depth-diff-gaussian-rasterization&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            </br>
                            Add many extensions to vanilla Gaussian rasterization pipeline
                            used in 3D Gaussian Splatting, including depth forward pass, backward pass, and 4-th SH.
                            <!--                            </br>-->
                            <a href="https://github.com/ingra14m/depth-diff-gaussian-rasterization">Code</a>
                        </p>

                        <p align="justify">
                            <para><b>2. Awesome-Inverse-Rendering</b></para>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=Awesome-Inverse-Rendering&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            </br>
                            A collection of papers on NeRF-Based Inverse Rendering.
                            <!--                            </br>-->
                            <a href="https://github.com/ingra14m/Awesome-Inverse-Rendering">Code</a>
                        </p>

                        <p align="justify">
                            <para><b>3. floater-free-gaussian-splatting</b></para>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=floater-free-gaussian-splatting&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            </br>
                            Fix the densification bug to eliminate floaters in 3D-GS.
                            <!--                            </br>-->
                            <a href="https://github.com/ingra14m/floater-free-gaussian-splatting">Code</a>
                        </p>

                        <p align="justify">
                            <para><b>4. My-exp-Gaussian</b></para>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=My-exp-Gaussian&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            </br>
                            Early attempt to model specular highlights with ASG.
                            <!--                            </br>-->
                            <a href="https://github.com/ingra14m/My-exp-Gaussian">Code</a>
                        </p>

                        <!-- I was a research intern in different companies,
including <a href="https://www.megvii.com/megvii_research">Megvii Research Center</a> of <a href="https://www.megvii.com">Megvii</a> (Beijing, China)(July. 2019 - Sep. 2019).
<a href="">Noah's Ark Lab</a> of <a href="https://www.huawei.com/en/">Huawei</a> (Beijing, China)(Nov. 2020 - April. 2021) and DJI.
In my undergraduate study, I was an team member of computer vision group in <a href="https://baike.baidu.com/item/å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ç«žæŠ€æœºå™¨äººé˜Ÿ">
Harbin Institute of Technology Competition Robotics Team (HITCRT)</a>, named I Hiter (Harbin, China). -->
                        <!--</br></br>-->
                        <!--<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> -->
                        <!-- </p> -->
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 8 -->
            <!--                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                            <tbody>-->
            <!--                            <tr>-->
            <!--                                <td>-->
            <!--                                    <heading>Honors</heading>-->
            <!--                                    <p>Apr. 2022, Hong Kong PhD Fellowship Scheme - Research Grants Council (RGC) of-->
            <!--                                        Hong Kong</p>-->
            <!--                                    <p>Nov. 2019, Academic scholarship - Zhejiang University </p>-->
            <!--                                    <p>Jun. 2019, Outstanding Graduate - Harbin Institute of Technology </p>-->
            <!--                                    <p>Jun. 2019, Top 100 excellent graduation thesis - Harbin Institute of Technology-->
            <!--                                    </p>-->
            <!--                                    <p>Mar. 2019, First price of innovation scholarship of MIIT - Harbin Institute of-->
            <!--                                        Technology</p>-->
            <!--                                    <p>Jan. 2019, Top 10 College Student in Harbin Institute of Technology - Harbin-->
            <!--                                        Institute of-->
            <!--                                        Technology</p>-->
            <!--                                    <p>Mar. 2018, Outstanding student in Hei Longjiang Province - Harbin Institute of-->
            <!--                                        Technology</p>-->
            <!--                                    <p>Oct. 2017, SMC Scholarship - Harbin Institute of Technology</p>-->
            <!--                                    <p>Oct. 2016, National Scholarship - Harbin Institute of Technology</p>-->
            <!--                                </td>-->
            <!--                            </tr>-->
            <!--                            </tbody>-->
            <!--                        </table>-->

            <!--SECTION 8 -->
            <!--                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                            <tbody>-->
            <!--                            <tr>-->
            <!--                                <td>-->
            <!--                                    <heading>About Me</heading>-->
            <!--                                    <p><strong>Skills</strong>: Python / C / C ++ / Matlab, PyTorch, Linux, ROS, OpenCV-->
            <!--                                    </p>-->
            <!--                                </td>-->
            <!--                            </tr>-->
            <!--                            </tbody>-->
            <!--                        </table>-->

            <!--                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                            <tbody>-->
            <!--                            <tr>-->
            <!--                                <td width="100%" align="middle">-->
            <!--                                    <p align="center" style="width: 25% ">-->
            <!--                                        <script type="text/javascript" id="clstr_globe"-->
            <!--                                                src="//clustrmaps.com/globe.js?d=3LiPnVFG5d-0xCFz-Mksb8GtJ_xg3bHvDe_e6PIrMAQ"></script>-->
            <!--                                    </p>-->
            <!--                                </td>-->
            <!--                            </tr>-->
            <!--                            </tbody>-->
            <!--                        </table>-->

            <!--SECTION 10 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td><br>
                        <p align="right">
                            <font size="2"> Last update: 2024.10.09. <a
                                    href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


        </td>
    </tr>
    </tbody>
</table>
</body>

</html>
