<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=950">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        para {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 18px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Ziyi Yang</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/png" href="./images/hku.png">
</head>

<body>
<table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <name>Ziyi Yang</name>
                        </p>

                        I am a master student at Zhejiang University advised by professor <a
                            href="http://www.cad.zju.edu.cn/home/jin/">Xiaogang
                        Jin</a>.
                        My research lies at the neural rendering, inverse rendering, and computer graphics
                        <!--  (Please don't say I'm a computer vision researcher. I am truly a graphics enthusiast lol ðŸ˜‚). -->
                        I am currently interested in 3D Gaussian Splatting (But I am not optimistic about it).
                        Now, I am having a very pleasant time at ByteDance MMLab as a research intern.
                        Prior to joining ZJU, I got my Bachelor's Degree from <a href="https://en.sjtu.edu.cn/">Shanghai
                        Jiaotong University</a> in 2022.

                        </br></br>
                        <strong><font color="red">I'm now looking for a 25 Fall Ph.D. position in computer graphics / 3D
                            vision! Welcome to contact me! </font></strong>

                        </br>
                        </p>
                        <p align="center">
                            <a href="mailto:14ziyiyang@gmail.com"> Email </a> /
                            <a href="https://scholar.google.com/citations?user=B0IyfqQAAAAJ&hl=en">
                                Google Scholar </a> /
                            <a href="https://github.com/ingra14m"> Github </a>
                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./images/yzy.jpg"
                                           style="width: 150; height: 225;"></td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Recent News</heading>
                    </td>
                </tr>
                </tbody>
            </table>
            <div style="height:170px;overflow-y:scroll;">
                <ul>
                    <li>[2024.02] two papers have been accepted by the <b>CVPR 2024</b>.
                    </li>
                </ul>
                <ul>
                    <li>[2023.12] one paper has been accepted by the <b>AAAI 2024</b>.
                    </li>
                </ul>
            </div>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <td>
                    <heading>Publications</heading>
                </td>
                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="20%"><img src="./publications/2024/Spec-Gaussian/teaser.png" alt="PontTuset"
                                         width="300" height="160" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://ingra14m.github.io/Spec-Gaussian-website/">
                            <papertitle>Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=Specular-Gaussians&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a><br><strong>Ziyi Yang</strong>, Xinyu Gao, Yang-Tian Sun, Yi-Hua Huang, Xiaoyang Lyu, Wen
                            Zhou, Shaohui Jiao, Xiaojuan Qi, Xiaogang Jin<sup>&dagger;</sup>
                            <!--                                <a href="https://xjqi.github.io/">Xiaojuan Qi</a>-->
                            <br>
                            <em>Arxiv</em>, 2024.
                            <br>
                            <a href="https://arxiv.org/abs/2402.15870">Paper</a> /
                            <a href="https://ingra14m.github.io/Spec-Gaussian-website/">Project Page</a> /
                            <a href="https://github.com/ingra14m/Specular-Gaussians">Code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">EscherNet is a multi-view conditioned
                            diffusion model for view synthesis. EscherNet learns implicit and generative 3D
                            representations coupled with the camera positional encoding (CaPE), allowing
                            continuous relative camera control between an arbitrary number of reference and
                            target views.</p>
                    </td>
                </tr>

                <tr>
                    <td width="20%"><img src="./publications/2024/SC-GS/teaser.png" alt="PontTuset" width="300"
                                         height="111" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://yihua7.github.io/SC-GS-web/">
                            <papertitle>SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic
                                Scenes
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=yihua7&repo=SC-GS&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>

                            <br>Yi-Hua Huang<sup>*</sup>, Yang-Tian Sun<sup>*</sup>, <strong>Ziyi
                                Yang<sup>*</sup></strong>, Xiaoyang Lyu,
                            Yan-Pei Cao<sup>&dagger;</sup>, Xiaojuan Qi<sup>&dagger;</sup>
                            <br>
                            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference
                                (<strong>CVPR</strong>)</em>,
                            2024. Seattle WA, USA.
                            <br>
                            <a href="https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf">Paper</a>
                            /
                            <a href="https://yihua7.github.io/SC-GS-web/">Project Page</a> /
                            <a href="https://github.com/yihua7/SC-GS">Code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px"> We propose a new representation that
                            explicitly decomposes the motion
                            and appearance of dynamic scenes into sparse control points and dense Gaussians,
                            respectively.
                            Our key idea is to use sparse control points, significantly fewer in number than
                            the Gaussians,
                            to learn compact 6 DoF transformation bases, which can be locally interpolated
                            through learned interpolation weights
                            to yield the motion field of 3D Gaussians. Please visit project page for more
                            demos.</p>
                    </td>
                </tr>

                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="25%"><img src="./publications/2024/Deformable-GS/teaser.png" alt="PontTuset" width="300"
                                         height="169" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://github.com/ingra14m/Deformable-3D-Gaussians">
                            <papertitle>Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=Deformable-3D-Gaussians&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br><strong>Ziyi Yang</strong>, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, Xiaogang
                            Jin<sup>&dagger;</sup>
                            <br>
                            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference
                                (<strong>CVPR</strong>)</em>,
                            2024. Seattle WA, USA.
                            <br>
                            <a href="https://arxiv.org/abs/2309.13101">arXiv</a> /
                            <a href="https://ingra14m.github.io/Deformable-Gaussians/">Project Page</a> /
                            <a href="https://github.com/ingra14m/Deformable-3D-Gaussians">Code</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We have analyzed the constraints present
                            in current neural scene representation techniques with geometry priors,
                            and have identified issues in their ability to reconstruct detailed structures
                            due to a biased optimization towards high
                            color intensities and the complex SDF distribution. As a result, we have
                            developed a feature rendering scheme that balances color regions and
                            have implemented a hybrid representation to address the limitations of the SDF
                            distribution.</p>
                    </td>
                </tr>


                <tr>
                    <td width="20%"><img src="./publications/2024/sire-ir/teaser.png" alt="PontTuset"
                                         width="300" height="180" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://github.com/ingra14m/SIRe-IR">
                            <papertitle>SIRe-IR: Inverse Rendering for BRDF Reconstruction with Shadow and Illumination
                                Removal in High-Illuminance Scenes
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=ingra14m&repo=SIRe-IR&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br><strong>Ziyi Yang</strong>, Yanzhen Chen, Xinyu Gao, Yazhen Yuan, Yu Wu, Xiaowei Zhou,
                            Xiaogang Jin<sup>&dagger;</sup>
                            <br>
                            <em>Arxiv</em>, 2023.
                            <br>
                            <a href="">Project Page</a> /
                            <a href="https://arxiv.org/abs/2310.13030">arXiv</a> /
                            <a href="https://github.com/ingra14m/SIRe-IR">code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We develop a hybrid neural rendering model
                            that makes
                            image-based
                            representation and neural 3D representation join forces to render high-quality
                            and view-consistent
                            images.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="./publications/2024/agif/teaser.jpeg" alt="PontTuset" width="300"
                                         height="168" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="">
                            <papertitle>A General Implicit Framework for Fast NeRF Composition and Rendering
                            </papertitle>
                        </a>
                            <br>Xinyu Gao, <strong>Ziyi Yang</strong>, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin<sup>&dagger;</sup>,
                            Changqing Zou<sup>&dagger;</sup>
                            <br>
                            <em>The 38th AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>,
                            2024. Vancouver, Canada.
                            <br>
                            <a href="https://arxiv.org/abs/2308.04669">ArXiv</a>
                            <!--                            <a href="https://github.com/CVMI-Lab/Speech2Lip">Code</a>-->
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a novel decomposition
                            synthesis-composition
                            framework called Speech2Lip for
                            high-fidelity talking head video synthesis, which disentangles speech-sensitive
                            and
                            speech-insensitive motions/appearances. </p>
                    </td>
                </tr>
                <tbody>
            </table>

            <!-- SECTION 5 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Open-source Contribution</heading>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 6 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="25%"><img src="./images/ICRA18/robots.jpg" alt="PontTuset" width="300"
                                         style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://www.robomaster.com/en-US/resource/pages/announcement/728">
                            <papertitle>ICRA 2018 DJI RoboMaster AI Challenge</papertitle>
                        </a>
                            <br><a href="./images/ICRA18/team.jpg">Team: I Hiter</a>. Xingguang Zhong, Xin
                            Kong,
                            <strong>Xiaoyang Lyu</strong>, Le Qi, Hao Huang, Linrui Tian, Songwei Li
                            <br>
                            <em> IEEE International Conference on Robotics and Automation
                                (<strong>ICRA</strong>)</em>,
                            2018. Brisbane, Australia.
                            <br>
                            <a href="./images/ICRA18/champion.jpg"><strong>Global Champion</strong> </a> /
                            <a href="./images/ICRA18/ranking.jpg">Ranking: <strong>1st</strong>/21 </a>/
                            <a href="./images/ICRA18/certificate.jpg">Certificate</a> /
                            <a href="/images/ICRA18/ICRA18-DJI.mp4">Video</a> /
                            <a href="./images/ICRA18/rules.pdf">Rules</a>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">Our team built two fully automatic robots,
                            including <a href="./images/ICRA18/IHiter-tech.pdf">
                                machinery, circuit, control and algorithm</a>. I was responsible for visual
                            servo, target
                            detection, target localization and
                            decision-making of robots.</p>
                    </td>
                </tr>

                <tbody>
                <tr>
                    <td width="25%"><img src="./images/RM17/autofightback.gif" alt="PontTuset" width="300"
                                         style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://www.robomaster.com/en-US/robo/history">
                            <papertitle>2017, 2018, 2019 RoboMaster Robotics Competition</papertitle>
                        </a>
                            <br><a href="./images/RM17/team.jpg">Team: I Hiter</a>. Wei Chen, Xin Kong,
                            <strong>Xiaoyang
                                Lyu</strong>, etc.
                            <br>
                            <em>China University Robot Competition (å…¨å›½å¤§å­¦ç”Ÿæœºå™¨äººå¤§èµ›)</em>, 2017, 2018, 2019.
                            Shenzhen, China.
                            <br>
                            <a href="./images/RM17/first-prize.bmp"><strong>First Prize</strong> </a> /
                            <a href="https://en.wikipedia.org/wiki/RoboMaster#Winners">Ranking:
                                <strong>4th</strong>/200+
                            </a> in 2017, 2018, <a
                                    href="https://www.robomaster.com/en-US/resource/pages/announcement/1035"><strong>6th</strong>/200+
                                in 2019.</a>/
                            <a href="./images/RM17/rm_lxy.png">Certificate</a> /
                            <a href="https://www.bilibili.com/video/av27846163/">Highlights</a>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">Our team built more than <a
                                href="https://www.bilibili.com/video/av23489177/">10 complex automatic or
                            semi-automatic
                            robots</a> every year.
                            In 2017, I was mainly responsible for building and <a
                                    href="images/RM17/manipulator.jfif">manipulating</a> Engineering Robot. In
                            2018, I was
                            responsible for
                            <a href="./images/RM17/visual-servo.mp4">visual servo</a>, which involves
                            computer vision and
                            machine learning. In 2019, I became the leader of computer vision group and the
                            coach of our team.
                        </p>
                    </td>
                </tr>
            </table>

            <!-- SECTION 5 -->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tbody><tr>
  <td><heading>Projects</heading>
  </td>
  </tr></tbody>
</table> -->

            <!--SECTION 6 -->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

<tbody><tr>
    <td width="20%"><img src="./images/ST18/strawberry.gif" alt="PontTuset" width="250" style="border-style: none"></td>
    <td width="80%" valign="top">
        <papertitle>Flowing Strawberry Picking Robot</papertitle>
        <br><strong>Xiaoyang Lyu</strong>, Le Qi, Linrui Tian, Songwei Li
        <br>
        <em> Undergraduate Mechanical Innovation Competition.
        <br>

        <strong>First Prize</strong> /
        <a href="./images/ST18/certificates.jpg">Certificate</a>
        </p><p></p>
        <p align="justify" style="font-size:13px"> Our group build a strawberry picking robot,
            which can detect strawberries and pick them automatically. I responsible for designing
            a machine learning algorithm which can detect strawberry on embedded platform.</td>
</tr>

</table> -->

            <!--SECTION 7 -->
            <!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td>-->
            <!--                        <heading>Internship</heading>-->
            <!--                        <p align="justify">-->
            <!--                            <para>DJI ( Flying System )</para>-->
            <!--                            </br>-->
            <!--                            Topic: Reconstruction from neural rendering.-->
            <!--                            </br>-->
            <!--                            Mentor: <a-->
            <!--                                href="https://scholar.google.com/citations?user=29otYD8AAAAJ&hl=zh-CN">Yi-->
            <!--                            Lin</a>-->
            <!--                            </br>-->
            <!--                            Time: Apr. 2022 - Aug. 2022-->
            <!--                        </p>-->

            <!--                        <p align="justify">-->
            <!--                            <para>Noah's Ark Lab</para>-->
            <!--                            </br>-->
            <!--                            Topic: Visual mapping for autonomous driving.-->
            <!--                            </br>-->
            <!--                            Mentor: <a-->
            <!--                                href="https://scholar.google.com/citations?user=7p462ysAAAAJ&hl=zh-CN">Wanlong-->
            <!--                            Li</a>-->
            <!--                            </br>-->
            <!--                            Time: Nov. 2020 - Apr. 2021.-->
            <!--                        </p>-->

            <!--                        <p align="justify">-->
            <!--                            <para>Megvii Research Center</para>-->
            <!--                            </br>-->
            <!--                            Topic: Self-supervised depth estimation for autonomous driving.-->
            <!--                            </br>-->
            <!--                            Mentor: <a href="https://cn.linkedin.com/in/xuziyao">Ziyao Xu</a>-->
            <!--                            </br>-->
            <!--                            Time: July. 2019 - Sep. 2019.-->
            <!--                        </p>-->
            <!--                        &lt;!&ndash; I was a research intern in different companies,-->
            <!--including <a href="https://www.megvii.com/megvii_research">Megvii Research Center</a> of <a href="https://www.megvii.com">Megvii</a> (Beijing, China)(July. 2019 - Sep. 2019).-->
            <!--<a href="">Noah's Ark Lab</a> of <a href="https://www.huawei.com/en/">Huawei</a> (Beijing, China)(Nov. 2020 - April. 2021) and DJI.-->
            <!--In my undergraduate study, I was an team member of computer vision group in <a href="https://baike.baidu.com/item/å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ç«žæŠ€æœºå™¨äººé˜Ÿ">-->
            <!--Harbin Institute of Technology Competition Robotics Team (HITCRT)</a>, named I Hiter (Harbin, China). &ndash;&gt;-->
            <!--                        &lt;!&ndash;</br></br>&ndash;&gt;-->
            <!--                        &lt;!&ndash;<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> &ndash;&gt;-->
            <!--                        &lt;!&ndash; </p> &ndash;&gt;-->
            <!--                    </td>-->
            <!--                </tr>-->
            <!--                </tbody>-->
            <!--            </table>-->

            <!--SECTION 8 -->
            <!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td>-->
            <!--                        <heading>Honors</heading>-->
            <!--                        <p>Apr. 2022, Hong Kong PhD Fellowship Scheme - Research Grants Council (RGC) of-->
            <!--                            Hong Kong</p>-->
            <!--                        <p>Nov. 2019, Academic scholarship - Zhejiang University </p>-->
            <!--                        <p>Jun. 2019, Outstanding Graduate - Harbin Institute of Technology </p>-->
            <!--                        <p>Jun. 2019, Top 100 excellent graduation thesis - Harbin Institute of Technology-->
            <!--                        </p>-->
            <!--                        <p>Mar. 2019, First price of innovation scholarship of MIIT - Harbin Institute of-->
            <!--                            Technology</p>-->
            <!--                        <p>Jan. 2019, Top 10 College Student in Harbin Institute of Technology - Harbin-->
            <!--                            Institute of-->
            <!--                            Technology</p>-->
            <!--                        <p>Mar. 2018, Outstanding student in Hei Longjiang Province - Harbin Institute of-->
            <!--                            Technology</p>-->
            <!--                        <p>Oct. 2017, SMC Scholarship - Harbin Institute of Technology</p>-->
            <!--                        <p>Oct. 2016, National Scholarship - Harbin Institute of Technology</p>-->
            <!--                    </td>-->
            <!--                </tr>-->
            <!--                </tbody>-->
            <!--            </table>-->

            <!--SECTION 8 -->
            <!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td>-->
            <!--                        <heading>About Me</heading>-->
            <!--                        <p><strong>Skills</strong>: Python / C / C ++ / Matlab, PyTorch, Linux, ROS, OpenCV-->
            <!--                        </p>-->
            <!--                    </td>-->
            <!--                </tr>-->
            <!--                </tbody>-->
            <!--            </table>-->

            <!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
            <!--                <tbody>-->
            <!--                <tr>-->
            <!--                    <td width="100%" align="middle">-->
            <!--                        <p align="center" style="width: 25% ">-->
            <!--                            <script type="text/javascript" id="clstr_globe"-->
            <!--                                    src="//clustrmaps.com/globe.js?d=3LiPnVFG5d-0xCFz-Mksb8GtJ_xg3bHvDe_e6PIrMAQ"></script>-->
            <!--                        </p>-->
            <!--                    </td>-->
            <!--                </tr>-->
            <!--                </tbody>-->
            <!--            </table>-->

            <!--SECTION 10 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td><br>
                        <p align="right">
                            <font size="2"> Last update: 2024.05.23. <a
                                    href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


        </td>
    </tr>
    </tbody>
</table>
</body>

</html>