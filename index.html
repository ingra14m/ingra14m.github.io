<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=950">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        para {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 18px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Ziyi Yang</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/png" href="./images/hku.png">
</head>

<body>
<table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <name>Ziyi Yang</name>
                        </p>

                        I am a master student at Zhejiang University advised by professor <a
                            href="http://www.cad.zju.edu.cn/home/jin/">Xiaogang
                        Jin</a>.
                        My research lies at the neural rendering, inverse rendering, and computer graphics
                        <!--  (Please don't say I'm a computer vision researcher. I am truly a graphics enthusiast lol ðŸ˜‚). -->
                        I am currently interested in 3D Gaussian Splatting (But I am not optimistic about it).
                        Now, I am having a very pleasant time at ByteDance MMLab as a research intern.
                        Prior to joining ZJU, I got my Bachelor's Degree from <a href="https://en.sjtu.edu.cn/">Shanghai
                        Jiaotong University</a> in 2022.

                        </br></br>
                        <strong><font color="red">I'm now looking for a 25 Fall Ph.D. position in computer graphics / 3D
                            vision! Welcome to contact me! </font></strong>

                        </br>
                        </p>
                        <p align="center">
                            <a href="mailto:14ziyiyang@gmail.com"> Email </a> /
                            <a href="https://scholar.google.com/citations?user=B0IyfqQAAAAJ&hl=en">
                                Google Scholar </a> /
                            <a href="https://github.com/ingra14m"> Github </a>
                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./images/yzy.jpg"
                                           style="width: 150; height: 225;"></td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Recent News</heading>
                    </td>
                </tr>
                </tbody>
            </table>
            <div style="height:170px;overflow-y:scroll;">
                <ul>
                    <li>[2024.02] two papers have been accepted by the <b>CVPR 2024</b>.
                    </li>
                </ul>
                <ul>
                    <li>[2023.12] one paper has been accepted by the <b>AAAI 2024</b>.
                    </li>
                </ul>
            </div>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <td>
                    <heading>Publications</heading>
                </td>
                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="20%"><img src="./images/CVPR24/EscherNet/teaser.png" alt="PontTuset"
                                         width="300" height="160" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://kxhit.github.io/EscherNet">
                            <papertitle>EscherNet: A Generative Model for Scalable View Synthesis
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=kxhit&repo=EscherNet&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>

                            <br><a href="https://kxhit.github.io/"> Xin Kong</a>,
                            <a href="http://shikun.io/">Shikun Liu</a>,
                            <strong>Xiaoyang Lyu</strong>, <a href="https://marwan99.github.io/">Marwan
                                Taher</a>, <a href="https://xjqi.github.io/">Xiaojuan Qi</a>, <a
                                    href="https://www.doc.ic.ac.uk/~ajd/">Andrew J. Davison</a>
                            <br>
                            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference
                                (<strong>CVPR</strong>)</em>,
                            2024. Seattle WA, USA.
                            <br>
                            <a href="./images/CVPR24/EscherNet/EscherNet.pdf">Paper</a> /
                            <a href="https://kxhit.github.io/EscherNet">Project Page</a> /
                            <a href="https://github.com/kxhit/EscherNet">Code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">EscherNet is a multi-view conditioned
                            diffusion model for view synthesis. EscherNet learns implicit and generative 3D
                            representations coupled with the camera positional encoding (CaPE), allowing
                            continuous relative camera control between an arbitrary number of reference and
                            target views.</p>
                    </td>
                </tr>

                <tr>
                    <td width="20%"><img src="./images/CVPR24/SC-GS/teaser.png" alt="PontTuset" width="300"
                                         height="111" style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://yihua7.github.io/SC-GS-web/">
                            <papertitle>SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic
                                Scenes
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=yihua7&repo=SC-GS&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>

                            <br>Yi-Hua Huang<sup>*</sup>, Yang-Tian Sun<sup>*</sup>, <strong>Ziyi
                                Yang<sup>*</sup></strong>, Xiaoyang Lyu,
                            Yan-Pei Cao<sup>&dagger;</sup>, Xiaojuan Qi<sup>&dagger;</sup>
                            <br>
                            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference
                                (<strong>CVPR</strong>)</em>,
                            2024. Seattle WA, USA.
                            <br>
                            <a href="https://yihua7.github.io/SC-GS-web/materials/SC_GS_Arxiv.pdf">Paper</a>
                            /
                            <a href="https://yihua7.github.io/SC-GS-web/">Project Page</a> /
                            <a href="https://github.com/yihua7/SC-GS">Code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px"> We propose a new representation that
                            explicitly decomposes the motion
                            and appearance of dynamic scenes into sparse control points and dense Gaussians,
                            respectively.
                            Our key idea is to use sparse control points, significantly fewer in number than
                            the Gaussians,
                            to learn compact 6 DoF transformation bases, which can be locally interpolated
                            through learned interpolation weights
                            to yield the motion field of 3D Gaussians. Please visit project page for more
                            demos.</p>
                    </td>
                </tr>

                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="25%"><img src="./images/ICCV23/Occ-SDF.gif" alt="PontTuset" width="300"
                                         height="169" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://arxiv.org/pdf/2303.09152">
                            <papertitle>Learning a Room with the Occ-SDF Hybrid: Signed Distance
                                Function Mingled with Occupancy Aids Scene Representation
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=shawlyu&repo=Occ-SDF-Hybrid&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br><strong>Xiaoyang Lyu</strong>, Peng Dai, Zizhang Li, etc.
                            <br>
                            <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>,
                            2023. Paris, France.
                            <br>
                            <a href="https://arxiv.org/pdf/2303.09152">arXiv</a> /
                            <a href="https://shawlyu.github.io/Occ-SDF-Hybrid/">Project Page</a> /
                            <a href="https://github.com/shawLyu/Occ-SDF-Hybrid">Code</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We have analyzed the constraints present
                            in current neural scene representation techniques with geometry priors,
                            and have identified issues in their ability to reconstruct detailed structures
                            due to a biased optimization towards high
                            color intensities and the complex SDF distribution. As a result, we have
                            developed a feature rendering scheme that balances color regions and
                            have implemented a hybrid representation to address the limitations of the SDF
                            distribution.</p>
                    </td>
                </tr>
                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="25%"><img src="./images/ICCV23/RICO.png" alt="PontTuset" width="280"
                                         height="280" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://arxiv.org/pdf/2303.08605">
                            <papertitle>RICO: Regularizing the Unobservable for Indoor Compositional
                                Reconstruction
                            </papertitle>
                        </a>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=kyleleey&repo=RICO&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            <br>Zizhang Li, <strong>Xiaoyang Lyu</strong>, etc.
                            <br>
                            <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>,
                            2023.
                            Paris, France.
                            <br>
                            <a href="https://arxiv.org/pdf/2303.08605">arXiv</a> /
                            <a href="https://github.com/kyleleey/RICO">Code</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We have presented RICO, a novel approach
                            for
                            compositional reconstruction in indoor scenes.
                            Our key motivation is to regularize the unobservable regions for the objects
                            with
                            partial observations in indoor scenes.
                            We exploit the geometry smoothness for the occluded background, and then adopt
                            the
                            improved background as the prior to regularize the objectsâ€™ geometry.</p>
                    </td>
                </tr>
                <tr>
                    <td width="25%"><img src="./images/ICCV23/Speech2Lip.gif" alt="PontTuset" width="300"
                                         height="168" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="">
                            <papertitle>Speech2Lip: High-fidelity Speech to Lip Generation by Learning
                                from a Short Video
                            </papertitle>
                        </a>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=CVMI-Lab&repo=Speech2Lip&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            <br>Xiuzhe Wu, Pengfei Hu, Yang Wu, <strong>Xiaoyang Lyu</strong>, etc.
                            <br>
                            <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>,
                            2023. Paris, France.
                            <br>
                            <a href="https://arxiv.org/pdf/2309.04814.pdf">ArXiv</a> /
                            <a href="https://github.com/CVMI-Lab/Speech2Lip">Code</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a novel decomposition
                            synthesis-composition
                            framework called Speech2Lip for
                            high-fidelity talking head video synthesis, which disentangles speech-sensitive
                            and
                            speech-insensitive motions/appearances. </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="./images/CVPR23/HybridPipeline.png" alt="PontTuset"
                                         width="300" height="130" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a
                                href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dai_Hybrid_Neural_Rendering_for_Large-Scale_Scenes_With_Motion_Blur_CVPR_2023_paper.pdf">
                            <papertitle>Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur
                            </papertitle>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=CVMI-Lab&repo=HybridNeuralRendering&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                        </a>
                            <br>Peng Dai, Yinda Zhang, Xin Yu, <strong>Xiaoyang Lyu</strong>, Xiaojuan Qi
                            <br>
                            <em>Conference on Computer Vision and Pattern
                                Recognition(<strong>CVPR</strong>)</em>, 2023.
                            Vancouver, Canada.
                            <br>
                            <a href="https://daipengwa.github.io/Hybrid-Rendering-ProjectPage/">Project
                                Page</a> /
                            <a href="https://arxiv.org/pdf/2304.12652.pdf">arXiv</a> /
                            <a href="https://github.com/CVMI-Lab/HybridNeuralRendering">code</a>

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We develop a hybrid neural rendering model
                            that makes
                            image-based
                            representation and neural 3D representation join forces to render high-quality
                            and view-consistent
                            images.</p>
                    </td>
                </tr>
                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="25%"><img src="./images/ICRA23/ENIRUL.gif" alt="PontTuset" width="300"
                                         height="160" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://ieeexplore.ieee.org/iel7/10160211/10160212/10160322.pdf">
                            <papertitle>Efficient Implicit Neural Reconstruction Using LiDAR
                            </papertitle>
                        </a>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=StarRealMan&repo=EINRUL&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            <br>Dongyu Yan, <strong>Xiaoyang Lyu</strong>, Jieqi Shi, Yi Lin
                            <br>
                            <em>IEEE International Conference on Robotics and Automation
                                (<strong>ICRA</strong>)</em>, 2023.
                            London, UK.
                            <br>
                            <a href="https://starydy.xyz/EINRUL/">Project Page</a> /
                            <a href="https://arxiv.org/abs/2302.14363">arXiv</a> /
                            <a href="https://github.com/StarRealMan/EINRUL">code</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a new method that uses sparse
                            LiDAR point clouds
                            and rough
                            odometry to reconstruct fine-grained implicit occupancy field efficiently within
                            a few minutes.
                            We introduce a new loss function that supervises directly in 3D space without 2D
                            rendering, avoiding
                            information loss.
                        </p>
                    </td>
                </tr>

                <tr bgcolor="#ffffd0" data-darkreader-inline-bgcolor=""
                    style="--darkreader-inline-bgcolor: #3b3b00;">
                    <td width="25%"><img src="./images/AAAI21/hr_depth.gif" alt="PontTuset" width="300"
                                         height="130" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://aaai.org/Conferences/AAAI-21/">
                            <papertitle>HR-Depth : High Resolution Self-Supervised Monocular Depth
                                Estimation
                            </papertitle>
                        </a>
                            <iframe
                                    src="https://ghbtns.com/github-btn.html?user=shawlyu&repo=HR-Depth&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="150" height="20"
                                    title="GitHub"></iframe>
                            <br><strong>Xiaoyang Lyu</strong>, Liang Liu, Mengmeng Wang, Xin Kong, etc.
                            <br>
                            <em>The 35th AAAI Conference on Artificial Intelligence
                                (<strong>AAAI</strong>)</em>, 2021. Virtual.
                            <br>
                            <a href="https://arxiv.org/abs/2012.07356">arXiv</a> /
                            <a href="https://github.com/shawLyu/HR-Depth">code(Training and more will
                                come)</a>
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">Based on theoretical and empirical
                            evidence, we present
                            HR-Depth, for
                            high-resolution self-supervised monocular depth estimation.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="./images/AAAI21/FCFR_pipeline.png" alt="PontTuset" width="300"
                                         height="130" style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://aaai.org/Conferences/AAAI-21/">
                            <papertitle>FCFR-Net: Frature Fusion based Coarse-to-Fine Residual Learning
                                for Depth Completion
                            </papertitle>
                        </a>
                            <br>Lina Liu, Xibin Song, <strong>Xiaoyang Lyu</strong>, Junwei Diao, etc.
                            <br>
                            <em>The 35th AAAI Conference on Artificial Intelligence
                                (<strong>AAAI</strong>)</em>, 2021. Virtual.
                            <br>
                            <a href="https://arxiv.org/abs/2012.08270">arXiv</a> /
                            code
                            <!-- <a href="https://arxiv.org/abs/2012.07356">arXiv</a> /
        <a href="https://github.com/shawLyu/HR-Depth">code</a> -->
                            <!--                <a href="https://www.youtube.com/watch?v=H_uf-KEFA9s">presentation</a>-->

                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">We propose a novel end-to-end residual
                            learning framework,
                            which formulates the depth completion as a tow-stage learning task. </p>
                    </td>
                </tr>
                <tbody>
            </table>

            <!-- SECTION 5 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Competitions</heading>
                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 6 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="25%"><img src="./images/ICRA18/robots.jpg" alt="PontTuset" width="300"
                                         style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://www.robomaster.com/en-US/resource/pages/announcement/728">
                            <papertitle>ICRA 2018 DJI RoboMaster AI Challenge</papertitle>
                        </a>
                            <br><a href="./images/ICRA18/team.jpg">Team: I Hiter</a>. Xingguang Zhong, Xin
                            Kong,
                            <strong>Xiaoyang Lyu</strong>, Le Qi, Hao Huang, Linrui Tian, Songwei Li
                            <br>
                            <em> IEEE International Conference on Robotics and Automation
                                (<strong>ICRA</strong>)</em>,
                            2018. Brisbane, Australia.
                            <br>
                            <a href="./images/ICRA18/champion.jpg"><strong>Global Champion</strong> </a> /
                            <a href="./images/ICRA18/ranking.jpg">Ranking: <strong>1st</strong>/21 </a>/
                            <a href="./images/ICRA18/certificate.jpg">Certificate</a> /
                            <a href="/images/ICRA18/ICRA18-DJI.mp4">Video</a> /
                            <a href="./images/ICRA18/rules.pdf">Rules</a>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">Our team built two fully automatic robots,
                            including <a href="./images/ICRA18/IHiter-tech.pdf">
                                machinery, circuit, control and algorithm</a>. I was responsible for visual
                            servo, target
                            detection, target localization and
                            decision-making of robots.</p>
                    </td>
                </tr>

                <tbody>
                <tr>
                    <td width="25%"><img src="./images/RM17/autofightback.gif" alt="PontTuset" width="300"
                                         style="border-style: none"></td>
                    <td width="75%" valign="top">
                        <p><a href="https://www.robomaster.com/en-US/robo/history">
                            <papertitle>2017, 2018, 2019 RoboMaster Robotics Competition</papertitle>
                        </a>
                            <br><a href="./images/RM17/team.jpg">Team: I Hiter</a>. Wei Chen, Xin Kong,
                            <strong>Xiaoyang
                                Lyu</strong>, etc.
                            <br>
                            <em>China University Robot Competition (å…¨å›½å¤§å­¦ç”Ÿæœºå™¨äººå¤§èµ›)</em>, 2017, 2018, 2019.
                            Shenzhen, China.
                            <br>
                            <a href="./images/RM17/first-prize.bmp"><strong>First Prize</strong> </a> /
                            <a href="https://en.wikipedia.org/wiki/RoboMaster#Winners">Ranking:
                                <strong>4th</strong>/200+
                            </a> in 2017, 2018, <a
                                    href="https://www.robomaster.com/en-US/resource/pages/announcement/1035"><strong>6th</strong>/200+
                                in 2019.</a>/
                            <a href="./images/RM17/rm_lxy.png">Certificate</a> /
                            <a href="https://www.bilibili.com/video/av27846163/">Highlights</a>
                        </p>
                        <p></p>
                        <p align="justify" style="font-size:13px">Our team built more than <a
                                href="https://www.bilibili.com/video/av23489177/">10 complex automatic or
                            semi-automatic
                            robots</a> every year.
                            In 2017, I was mainly responsible for building and <a
                                    href="images/RM17/manipulator.jfif">manipulating</a> Engineering Robot. In
                            2018, I was
                            responsible for
                            <a href="./images/RM17/visual-servo.mp4">visual servo</a>, which involves
                            computer vision and
                            machine learning. In 2019, I became the leader of computer vision group and the
                            coach of our team.
                        </p>
                    </td>
                </tr>
            </table>

            <!-- SECTION 5 -->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tbody><tr>
  <td><heading>Projects</heading>
  </td>
  </tr></tbody>
</table> -->

            <!--SECTION 6 -->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

<tbody><tr>
    <td width="20%"><img src="./images/ST18/strawberry.gif" alt="PontTuset" width="250" style="border-style: none"></td>
    <td width="80%" valign="top">
        <papertitle>Flowing Strawberry Picking Robot</papertitle>
        <br><strong>Xiaoyang Lyu</strong>, Le Qi, Linrui Tian, Songwei Li
        <br>
        <em> Undergraduate Mechanical Innovation Competition.
        <br>

        <strong>First Prize</strong> /
        <a href="./images/ST18/certificates.jpg">Certificate</a>
        </p><p></p>
        <p align="justify" style="font-size:13px"> Our group build a strawberry picking robot,
            which can detect strawberries and pick them automatically. I responsible for designing
            a machine learning algorithm which can detect strawberry on embedded platform.</td>
</tr>

</table> -->

            <!--SECTION 7 -->
<!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td>-->
<!--                        <heading>Internship</heading>-->
<!--                        <p align="justify">-->
<!--                            <para>DJI ( Flying System )</para>-->
<!--                            </br>-->
<!--                            Topic: Reconstruction from neural rendering.-->
<!--                            </br>-->
<!--                            Mentor: <a-->
<!--                                href="https://scholar.google.com/citations?user=29otYD8AAAAJ&hl=zh-CN">Yi-->
<!--                            Lin</a>-->
<!--                            </br>-->
<!--                            Time: Apr. 2022 - Aug. 2022-->
<!--                        </p>-->

<!--                        <p align="justify">-->
<!--                            <para>Noah's Ark Lab</para>-->
<!--                            </br>-->
<!--                            Topic: Visual mapping for autonomous driving.-->
<!--                            </br>-->
<!--                            Mentor: <a-->
<!--                                href="https://scholar.google.com/citations?user=7p462ysAAAAJ&hl=zh-CN">Wanlong-->
<!--                            Li</a>-->
<!--                            </br>-->
<!--                            Time: Nov. 2020 - Apr. 2021.-->
<!--                        </p>-->

<!--                        <p align="justify">-->
<!--                            <para>Megvii Research Center</para>-->
<!--                            </br>-->
<!--                            Topic: Self-supervised depth estimation for autonomous driving.-->
<!--                            </br>-->
<!--                            Mentor: <a href="https://cn.linkedin.com/in/xuziyao">Ziyao Xu</a>-->
<!--                            </br>-->
<!--                            Time: July. 2019 - Sep. 2019.-->
<!--                        </p>-->
<!--                        &lt;!&ndash; I was a research intern in different companies,-->
<!--including <a href="https://www.megvii.com/megvii_research">Megvii Research Center</a> of <a href="https://www.megvii.com">Megvii</a> (Beijing, China)(July. 2019 - Sep. 2019).-->
<!--<a href="">Noah's Ark Lab</a> of <a href="https://www.huawei.com/en/">Huawei</a> (Beijing, China)(Nov. 2020 - April. 2021) and DJI.-->
<!--In my undergraduate study, I was an team member of computer vision group in <a href="https://baike.baidu.com/item/å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ç«žæŠ€æœºå™¨äººé˜Ÿ">-->
<!--Harbin Institute of Technology Competition Robotics Team (HITCRT)</a>, named I Hiter (Harbin, China). &ndash;&gt;-->
<!--                        &lt;!&ndash;</br></br>&ndash;&gt;-->
<!--                        &lt;!&ndash;<span class="highlight"><strong>Internship Position: </strong> If you're interested in ...</span> &ndash;&gt;-->
<!--                        &lt;!&ndash; </p> &ndash;&gt;-->
<!--                    </td>-->
<!--                </tr>-->
<!--                </tbody>-->
<!--            </table>-->

            <!--SECTION 8 -->
<!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td>-->
<!--                        <heading>Honors</heading>-->
<!--                        <p>Apr. 2022, Hong Kong PhD Fellowship Scheme - Research Grants Council (RGC) of-->
<!--                            Hong Kong</p>-->
<!--                        <p>Nov. 2019, Academic scholarship - Zhejiang University </p>-->
<!--                        <p>Jun. 2019, Outstanding Graduate - Harbin Institute of Technology </p>-->
<!--                        <p>Jun. 2019, Top 100 excellent graduation thesis - Harbin Institute of Technology-->
<!--                        </p>-->
<!--                        <p>Mar. 2019, First price of innovation scholarship of MIIT - Harbin Institute of-->
<!--                            Technology</p>-->
<!--                        <p>Jan. 2019, Top 10 College Student in Harbin Institute of Technology - Harbin-->
<!--                            Institute of-->
<!--                            Technology</p>-->
<!--                        <p>Mar. 2018, Outstanding student in Hei Longjiang Province - Harbin Institute of-->
<!--                            Technology</p>-->
<!--                        <p>Oct. 2017, SMC Scholarship - Harbin Institute of Technology</p>-->
<!--                        <p>Oct. 2016, National Scholarship - Harbin Institute of Technology</p>-->
<!--                    </td>-->
<!--                </tr>-->
<!--                </tbody>-->
<!--            </table>-->

            <!--SECTION 8 -->
<!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td>-->
<!--                        <heading>About Me</heading>-->
<!--                        <p><strong>Skills</strong>: Python / C / C ++ / Matlab, PyTorch, Linux, ROS, OpenCV-->
<!--                        </p>-->
<!--                    </td>-->
<!--                </tr>-->
<!--                </tbody>-->
<!--            </table>-->

<!--            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
<!--                <tbody>-->
<!--                <tr>-->
<!--                    <td width="100%" align="middle">-->
<!--                        <p align="center" style="width: 25% ">-->
<!--                            <script type="text/javascript" id="clstr_globe"-->
<!--                                    src="//clustrmaps.com/globe.js?d=3LiPnVFG5d-0xCFz-Mksb8GtJ_xg3bHvDe_e6PIrMAQ"></script>-->
<!--                        </p>-->
<!--                    </td>-->
<!--                </tr>-->
<!--                </tbody>-->
<!--            </table>-->

            <!--SECTION 10 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td><br>
                        <p align="right">
                            <font size="2"> Last update: 2024.05.23. <a
                                    href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


        </td>
    </tr>
    </tbody>
</table>
</body>

</html>